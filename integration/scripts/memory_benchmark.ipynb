{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yt1nx1qo6rl",
   "metadata": {},
   "source": [
    "# Memory Benchmark Interactive Notebook\n",
    "\n",
    "This notebook allows you to run memory benchmark workloads interactively, similar to `test_memory_benchmark.py` but with direct control over server management and workload execution.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Valkey Server**: Located at `/home/ubuntu/valkey/build/bin/valkey-server`\n",
    "2. **Search Module**: Built at `/home/ubuntu/valkey-search/.build-release/libsearch.so`\n",
    "3. **Python Dependencies**: `valkey-py` and optionally `psutil`, `matplotlib`, `pandas`\n",
    "\n",
    "## Key Differences from Test Framework\n",
    "\n",
    "- **Manual Server Management**: You explicitly start/stop the server\n",
    "- **No Test Fixtures**: Direct client and server management\n",
    "- **Interactive Execution**: Modify and run workloads on the fly\n",
    "- **Explicit Paths**: All paths are hardcoded for your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f02589c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valkey Search Root: /home/ubuntu/valkey-search\n",
      "Valkey Server Path: /home/ubuntu/valkey/build/bin/valkey-server\n",
      "Module Path: /home/ubuntu/valkey-search/.build-release/libsearch.so\n",
      "JSON Module Path: /home/ubuntu/valkey-search/.build-release/integration/valkey-json/.build-release/src/libjson.so\n",
      "✅ Valkey server found\n",
      "✅ Search module found\n",
      "✅ JSON module found\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Environment Setup and Module Paths\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import signal\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up paths explicitly\n",
    "VALKEY_SEARCH_ROOT = Path('/home/ubuntu/valkey-search')\n",
    "VALKEY_SERVER_PATH = '/home/ubuntu/valkey/build/bin/valkey-server'\n",
    "MODULE_PATH = '/home/ubuntu/valkey-search/.build-release/libsearch.so'\n",
    "\n",
    "# JSON module path - adjust if needed\n",
    "JSON_MODULE_PATH = str(VALKEY_SEARCH_ROOT / '.build-release/integration/valkey-json/.build-release/src/libjson.so')\n",
    "\n",
    "# Add valkey-search to Python path\n",
    "sys.path.insert(0, str(VALKEY_SEARCH_ROOT))\n",
    "sys.path.insert(0, str(VALKEY_SEARCH_ROOT / 'integration'))\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Valkey Search Root: {VALKEY_SEARCH_ROOT}\")\n",
    "print(f\"Valkey Server Path: {VALKEY_SERVER_PATH}\")\n",
    "print(f\"Module Path: {MODULE_PATH}\")\n",
    "print(f\"JSON Module Path: {JSON_MODULE_PATH}\")\n",
    "\n",
    "if not Path(VALKEY_SERVER_PATH).exists():\n",
    "    print(f\"❌ Valkey server not found at {VALKEY_SERVER_PATH}\")\n",
    "else:\n",
    "    print(\"✅ Valkey server found\")\n",
    "\n",
    "if not Path(MODULE_PATH).exists():\n",
    "    print(f\"❌ Search module not found at {MODULE_PATH}\")\n",
    "else:\n",
    "    print(\"✅ Search module found\")\n",
    "    \n",
    "if not Path(JSON_MODULE_PATH).exists():\n",
    "    print(f\"⚠️ JSON module not found at {JSON_MODULE_PATH}\")\n",
    "    print(\"JSON module is optional but recommended for full functionality\")\n",
    "else:\n",
    "    print(\"✅ JSON module found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xc9ce5onmcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import Required Libraries and Test Components\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import asyncio\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import valkey client\n",
    "from valkey import ResponseError\n",
    "from valkey.client import Valkey\n",
    "from valkey.asyncio import Valkey as AsyncValkey\n",
    "\n",
    "# Import test framework components\n",
    "try:\n",
    "    from hash_generator import (\n",
    "        HashKeyGenerator, HashGeneratorConfig, IndexSchema, FieldSchema,\n",
    "        FieldType, VectorFieldSchema, VectorAlgorithm, VectorMetric\n",
    "    )\n",
    "    from tags_builder import (\n",
    "        TagsConfig, TagDistribution, TagSharingConfig, TagSharingMode\n",
    "    )\n",
    "    from string_generator import (\n",
    "        LengthConfig, PrefixConfig, Distribution, StringType\n",
    "    )\n",
    "    print(\"✅ Successfully imported hash generator components\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Failed to import components: {e}\")\n",
    "    print(\"Make sure hash_generator.py, tags_builder.py, and string_generator.py are in the integration directory\")\n",
    "\n",
    "# Try to import psutil for memory monitoring\n",
    "try:\n",
    "    import psutil\n",
    "    PSUTIL_AVAILABLE = True\n",
    "    print(\"✅ psutil available for memory monitoring\")\n",
    "except ImportError:\n",
    "    PSUTIL_AVAILABLE = False\n",
    "    print(\"⚠️ psutil not available - memory monitoring will be limited\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ycrfa82j63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Valkey Server Management\n",
    "class ValkeyServerManager:\n",
    "    \"\"\"Manages Valkey server lifecycle for notebook testing\"\"\"\n",
    "    \n",
    "    def __init__(self, port=6379, working_dir=\"/tmp/valkey-notebook\"):\n",
    "        self.port = port\n",
    "        self.working_dir = Path(working_dir)\n",
    "        self.working_dir.mkdir(exist_ok=True)\n",
    "        self.process = None\n",
    "        self.client = None\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"Start Valkey server with search and JSON modules\"\"\"\n",
    "        if self.process:\n",
    "            print(\"Server already running\")\n",
    "            return\n",
    "            \n",
    "        # Create config file\n",
    "        config_path = self.working_dir / \"valkey.conf\"\n",
    "        config_content = f\"\"\"\n",
    "port {self.port}\n",
    "dir {self.working_dir}\n",
    "save \"\"\n",
    "enable-debug-command yes\n",
    "loadmodule {MODULE_PATH}\n",
    "\"\"\"\n",
    "        \n",
    "        # Add JSON module if available\n",
    "        if Path(JSON_MODULE_PATH).exists():\n",
    "            config_content = f\"\"\"\n",
    "port {self.port}\n",
    "dir {self.working_dir}\n",
    "save \"\"\n",
    "enable-debug-command yes\n",
    "loadmodule {JSON_MODULE_PATH}\n",
    "loadmodule {MODULE_PATH}\n",
    "\"\"\"\n",
    "        \n",
    "        with open(config_path, 'w') as f:\n",
    "            f.write(config_content)\n",
    "        print(f\"✅ Config file created at {config_path}\")\n",
    "        # Start server\n",
    "        cmd = [VALKEY_SERVER_PATH, str(config_path)]\n",
    "        self.process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(f\"Starting Valkey server with command: {' '.join(cmd)}\")\n",
    "        # Wait for server to start\n",
    "        time.sleep(100)\n",
    "        \n",
    "        # Test connection\n",
    "        try:\n",
    "            self.client = Valkey(host='localhost', port=self.port, decode_responses=True)\n",
    "            self.client.ping()\n",
    "            print(f\"✅ Valkey server started on port {self.port}\")\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to connect to server: {e}\")\n",
    "            self.stop()\n",
    "            raise\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop Valkey server\"\"\"\n",
    "        if self.client:\n",
    "            try:\n",
    "                self.client.close()\n",
    "            except:\n",
    "                pass\n",
    "            self.client = None\n",
    "            \n",
    "        if self.process:\n",
    "            self.process.terminate()\n",
    "            try:\n",
    "                self.process.wait(timeout=5)\n",
    "            except subprocess.TimeoutExpired:\n",
    "                self.process.kill()\n",
    "                self.process.wait()\n",
    "            self.process = None\n",
    "            print(\"✅ Valkey server stopped\")\n",
    "    \n",
    "    def get_client(self, decode_responses=True):\n",
    "        \"\"\"Get a new client connection\"\"\"\n",
    "        if not self.process:\n",
    "            raise RuntimeError(\"Server not running\")\n",
    "        return Valkey(host='localhost', port=self.port, decode_responses=decode_responses)\n",
    "    \n",
    "    def get_async_client(self, decode_responses=False):\n",
    "        \"\"\"Get an async client connection\"\"\"\n",
    "        if not self.process:\n",
    "            raise RuntimeError(\"Server not running\")\n",
    "        return AsyncValkey(host='localhost', port=self.port, decode_responses=decode_responses)\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.stop()\n",
    "\n",
    "# Create server manager instance\n",
    "server_manager = ValkeyServerManager(port=6380)  # Using 6380 to avoid conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ruq322qd4i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Config file created at /tmp/valkey-notebook/valkey.conf\n",
      "Starting Valkey server with command: /home/ubuntu/valkey/build/bin/valkey-server /tmp/valkey-notebook/valkey.conf\n",
      "❌ Failed to connect to server: name 'Valkey' is not defined\n",
      "✅ Valkey server stopped\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Valkey' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 4: Start/Stop Server Controls\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Start the server\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mserver_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# To stop the server when done:\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# server_manager.stop()\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mValkeyServerManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Test connection\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28mself\u001b[39m.client = \u001b[43mValkey\u001b[49m(host=\u001b[33m'\u001b[39m\u001b[33mlocalhost\u001b[39m\u001b[33m'\u001b[39m, port=\u001b[38;5;28mself\u001b[39m.port, decode_responses=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m.client.ping()\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Valkey server started on port \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.port\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Valkey' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 4: Start/Stop Server Controls\n",
    "# Start the server\n",
    "server_manager.start()\n",
    "\n",
    "# To stop the server when done:\n",
    "# server_manager.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ohemhd47ho9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Memory Benchmark Helper Functions\n",
    "@dataclass\n",
    "class MemorySnapshot:\n",
    "    \"\"\"Memory usage snapshot\"\"\"\n",
    "    used_memory: int\n",
    "    used_memory_human: str\n",
    "    used_memory_rss: int\n",
    "    used_memory_peak: int\n",
    "    used_memory_search: int = 0\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "\n",
    "def get_memory_info(client) -> MemorySnapshot:\n",
    "    \"\"\"Get current memory usage from Valkey\"\"\"\n",
    "    info = client.info('memory')\n",
    "    \n",
    "    # Get search-specific memory if available\n",
    "    search_memory = 0\n",
    "    try:\n",
    "        search_info = client.execute_command(\"SEARCH.MEMORY\")\n",
    "        if isinstance(search_info, list):\n",
    "            for i in range(0, len(search_info), 2):\n",
    "                if search_info[i] == b'total_memory_bytes' or search_info[i] == 'total_memory_bytes':\n",
    "                    search_memory = int(search_info[i + 1])\n",
    "                    break\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return MemorySnapshot(\n",
    "        used_memory=info['used_memory'],\n",
    "        used_memory_human=info['used_memory_human'],\n",
    "        used_memory_rss=info['used_memory_rss'],\n",
    "        used_memory_peak=info['used_memory_peak'],\n",
    "        used_memory_search=search_memory\n",
    "    )\n",
    "\n",
    "def format_bytes(bytes_val: int) -> str:\n",
    "    \"\"\"Format bytes to human readable string\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if bytes_val < 1024.0:\n",
    "            return f\"{bytes_val:.2f} {unit}\"\n",
    "        bytes_val /= 1024.0\n",
    "    return f\"{bytes_val:.2f} TB\"\n",
    "\n",
    "def calculate_memory_overhead(before: MemorySnapshot, after: MemorySnapshot, num_keys: int) -> Dict:\n",
    "    \"\"\"Calculate memory overhead per key\"\"\"\n",
    "    memory_diff = after.used_memory - before.used_memory\n",
    "    search_diff = after.used_memory_search - before.used_memory_search\n",
    "    \n",
    "    return {\n",
    "        'total_memory_used': memory_diff,\n",
    "        'total_memory_human': format_bytes(memory_diff),\n",
    "        'search_memory_used': search_diff,\n",
    "        'search_memory_human': format_bytes(search_diff),\n",
    "        'bytes_per_key': memory_diff / num_keys if num_keys > 0 else 0,\n",
    "        'search_bytes_per_key': search_diff / num_keys if num_keys > 0 else 0,\n",
    "        'num_keys': num_keys\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iuxc3nix97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Workload Runner\n",
    "class WorkloadRunner:\n",
    "    \"\"\"Run memory benchmark workloads\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.results = []\n",
    "        \n",
    "    async def run_async_workload(self, generator_config, num_keys, batch_size=1000):\n",
    "        \"\"\"Run workload using async client for better performance\"\"\"\n",
    "        async_client = server_manager.get_async_client()\n",
    "        \n",
    "        # Create index\n",
    "        index_name = generator_config.index_schema.name\n",
    "        create_cmd = self._build_create_index_command(generator_config.index_schema)\n",
    "        await async_client.execute_command(*create_cmd)\n",
    "        \n",
    "        # Generate and insert data\n",
    "        generator = HashKeyGenerator(generator_config)\n",
    "        \n",
    "        # Take memory snapshot before\n",
    "        before = get_memory_info(self.client)\n",
    "        \n",
    "        # Insert data in batches\n",
    "        inserted = 0\n",
    "        for batch_start in range(0, num_keys, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, num_keys)\n",
    "            batch_size_actual = batch_end - batch_start\n",
    "            \n",
    "            # Generate batch\n",
    "            batch_data = list(generator.generate_range(batch_start, batch_end))\n",
    "            \n",
    "            # Insert using pipeline\n",
    "            pipe = async_client.pipeline(transaction=False)\n",
    "            for key, hash_data in batch_data:\n",
    "                pipe.hset(key, mapping=hash_data)\n",
    "            \n",
    "            await pipe.execute()\n",
    "            inserted += batch_size_actual\n",
    "            \n",
    "            if inserted % 10000 == 0:\n",
    "                print(f\"Inserted {inserted}/{num_keys} keys...\")\n",
    "        \n",
    "        # Take memory snapshot after\n",
    "        after = get_memory_info(self.client)\n",
    "        \n",
    "        # Calculate overhead\n",
    "        overhead = calculate_memory_overhead(before, after, num_keys)\n",
    "        \n",
    "        # Get index info\n",
    "        index_info = await async_client.execute_command(\"SEARCH.INDEX\", index_name, \"INFO\")\n",
    "        \n",
    "        await async_client.close()\n",
    "        \n",
    "        return {\n",
    "            'config': generator_config,\n",
    "            'num_keys': num_keys,\n",
    "            'memory_overhead': overhead,\n",
    "            'index_info': self._parse_index_info(index_info)\n",
    "        }\n",
    "    \n",
    "    def run_sync_workload(self, generator_config, num_keys, batch_size=1000):\n",
    "        \"\"\"Run workload using sync client\"\"\"\n",
    "        # Create index\n",
    "        index_name = generator_config.index_schema.name\n",
    "        create_cmd = self._build_create_index_command(generator_config.index_schema)\n",
    "        self.client.execute_command(*create_cmd)\n",
    "        \n",
    "        # Generate and insert data\n",
    "        generator = HashKeyGenerator(generator_config)\n",
    "        \n",
    "        # Take memory snapshot before\n",
    "        before = get_memory_info(self.client)\n",
    "        \n",
    "        # Insert data in batches\n",
    "        inserted = 0\n",
    "        for batch_start in range(0, num_keys, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, num_keys)\n",
    "            batch_size_actual = batch_end - batch_start\n",
    "            \n",
    "            # Generate batch\n",
    "            batch_data = list(generator.generate_range(batch_start, batch_end))\n",
    "            \n",
    "            # Insert using pipeline\n",
    "            pipe = self.client.pipeline(transaction=False)\n",
    "            for key, hash_data in batch_data:\n",
    "                pipe.hset(key, mapping=hash_data)\n",
    "            \n",
    "            pipe.execute()\n",
    "            inserted += batch_size_actual\n",
    "            \n",
    "            if inserted % 10000 == 0:\n",
    "                print(f\"Inserted {inserted}/{num_keys} keys...\")\n",
    "        \n",
    "        # Take memory snapshot after\n",
    "        after = get_memory_info(self.client)\n",
    "        \n",
    "        # Calculate overhead\n",
    "        overhead = calculate_memory_overhead(before, after, num_keys)\n",
    "        \n",
    "        # Get index info\n",
    "        index_info = self.client.execute_command(\"SEARCH.INDEX\", index_name, \"INFO\")\n",
    "        \n",
    "        return {\n",
    "            'config': generator_config,\n",
    "            'num_keys': num_keys,\n",
    "            'memory_overhead': overhead,\n",
    "            'index_info': self._parse_index_info(index_info)\n",
    "        }\n",
    "    \n",
    "    def _build_create_index_command(self, schema):\n",
    "        \"\"\"Build SEARCH.CREATE command from IndexSchema\"\"\"\n",
    "        cmd = [\"SEARCH.CREATE\", schema.name]\n",
    "        \n",
    "        if schema.on_hash:\n",
    "            cmd.extend([\"ON\", \"HASH\"])\n",
    "        \n",
    "        if schema.prefixes:\n",
    "            cmd.extend([\"PREFIX\", len(schema.prefixes)])\n",
    "            cmd.extend(schema.prefixes)\n",
    "        \n",
    "        cmd.append(\"SCHEMA\")\n",
    "        \n",
    "        for field in schema.fields:\n",
    "            cmd.append(field.name)\n",
    "            \n",
    "            if isinstance(field, VectorFieldSchema):\n",
    "                cmd.append(\"VECTOR\")\n",
    "                cmd.append(field.algorithm.value)\n",
    "                cmd.append(str(field.dimension * 2 + 4))  # Number of args\n",
    "                cmd.extend([\n",
    "                    \"TYPE\", field.data_type,\n",
    "                    \"DIM\", str(field.dimension),\n",
    "                    \"DISTANCE_METRIC\", field.metric.value\n",
    "                ])\n",
    "            else:\n",
    "                cmd.append(field.type.value)\n",
    "                if field.separator:\n",
    "                    cmd.extend([\"SEPARATOR\", field.separator])\n",
    "        \n",
    "        return cmd\n",
    "    \n",
    "    def _parse_index_info(self, info):\n",
    "        \"\"\"Parse index info response\"\"\"\n",
    "        result = {}\n",
    "        i = 0\n",
    "        while i < len(info):\n",
    "            if isinstance(info[i], bytes):\n",
    "                key = info[i].decode()\n",
    "            else:\n",
    "                key = str(info[i])\n",
    "            \n",
    "            if i + 1 < len(info):\n",
    "                value = info[i + 1]\n",
    "                if isinstance(value, bytes):\n",
    "                    value = value.decode()\n",
    "                result[key] = value\n",
    "            i += 2\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up all data\"\"\"\n",
    "        self.client.flushall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ezmzb8qda5r",
   "metadata": {},
   "source": [
    "## Example Workloads\n",
    "\n",
    "Below are several example workloads demonstrating different tag sharing patterns and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dru9nujnyod",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Example 1 - Simple Tag Workload with Low Sharing\n",
    "# This creates tags with minimal sharing between keys\n",
    "\n",
    "# Configuration for low tag sharing\n",
    "low_sharing_config = HashGeneratorConfig(\n",
    "    index_schema=IndexSchema(\n",
    "        name=\"idx_low_sharing\",\n",
    "        fields=[\n",
    "            FieldSchema(name=\"tags\", type=FieldType.TAG, separator=\"|\")\n",
    "        ],\n",
    "        on_hash=True,\n",
    "        prefixes=[\"user:\"]\n",
    "    ),\n",
    "    tags_config=TagsConfig(\n",
    "        num_tags_per_key=(3, 5),  # 3-5 tags per key\n",
    "        unique_tags_range=(10000, 50000),  # Large pool of unique tags\n",
    "        distribution=TagDistribution.UNIFORM,\n",
    "        sharing_config=TagSharingConfig(\n",
    "            mode=TagSharingMode.LOW_SHARING,\n",
    "            shared_tags_ratio=0.1,  # Only 10% of tags are shared\n",
    "            max_keys_per_shared_tag=10\n",
    "        )\n",
    "    ),\n",
    "    prefix=\"user:\",\n",
    "    key_pattern=\"user:{id}\"\n",
    ")\n",
    "\n",
    "# Create workload runner\n",
    "runner = WorkloadRunner(server_manager.get_client())\n",
    "\n",
    "# Run the workload\n",
    "print(\"Running low sharing workload...\")\n",
    "result = runner.run_sync_workload(low_sharing_config, num_keys=10000)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Total memory used: {result['memory_overhead']['total_memory_human']}\")\n",
    "print(f\"Memory per key: {result['memory_overhead']['bytes_per_key']:.2f} bytes\")\n",
    "print(f\"Search memory: {result['memory_overhead']['search_memory_human']}\")\n",
    "print(f\"Index info: {result['index_info'].get('num_indexed_keys', 'N/A')} keys indexed\")\n",
    "\n",
    "# Clean up for next test\n",
    "runner.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ltiisl96p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Example 2 - High Tag Sharing Workload\n",
    "# This creates tags with high sharing between keys (social network style)\n",
    "\n",
    "high_sharing_config = HashGeneratorConfig(\n",
    "    index_schema=IndexSchema(\n",
    "        name=\"idx_high_sharing\",\n",
    "        fields=[\n",
    "            FieldSchema(name=\"tags\", type=FieldType.TAG, separator=\"|\")\n",
    "        ],\n",
    "        on_hash=True,\n",
    "        prefixes=[\"post:\"]\n",
    "    ),\n",
    "    tags_config=TagsConfig(\n",
    "        num_tags_per_key=(5, 10),  # 5-10 tags per key\n",
    "        unique_tags_range=(100, 500),  # Small pool of tags (high reuse)\n",
    "        distribution=TagDistribution.ZIPFIAN,  # Some tags much more popular\n",
    "        sharing_config=TagSharingConfig(\n",
    "            mode=TagSharingMode.HIGH_SHARING,\n",
    "            shared_tags_ratio=0.8,  # 80% of tags are shared\n",
    "            max_keys_per_shared_tag=1000\n",
    "        )\n",
    "    ),\n",
    "    prefix=\"post:\",\n",
    "    key_pattern=\"post:{id}\"\n",
    ")\n",
    "\n",
    "# Run the workload\n",
    "print(\"Running high sharing workload...\")\n",
    "result = runner.run_sync_workload(high_sharing_config, num_keys=10000)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Total memory used: {result['memory_overhead']['total_memory_human']}\")\n",
    "print(f\"Memory per key: {result['memory_overhead']['bytes_per_key']:.2f} bytes\")\n",
    "print(f\"Search memory: {result['memory_overhead']['search_memory_human']}\")\n",
    "print(f\"Index info: {result['index_info'].get('num_indexed_keys', 'N/A')} keys indexed\")\n",
    "\n",
    "# Compare with low sharing\n",
    "print(f\"\\nNote: High sharing typically uses less memory per key due to tag reuse\")\n",
    "\n",
    "runner.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00lx0tdnd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Example 3 - Async Workload with Prefix Sharing\n",
    "# This demonstrates using async for better performance with larger datasets\n",
    "\n",
    "# Configuration with prefix sharing\n",
    "prefix_sharing_config = HashGeneratorConfig(\n",
    "    index_schema=IndexSchema(\n",
    "        name=\"idx_prefix_sharing\",\n",
    "        fields=[\n",
    "            FieldSchema(name=\"category\", type=FieldType.TAG, separator=\"|\"),\n",
    "            FieldSchema(name=\"tags\", type=FieldType.TAG, separator=\"|\")\n",
    "        ],\n",
    "        on_hash=True,\n",
    "        prefixes=[\"item:\"]\n",
    "    ),\n",
    "    tags_config=TagsConfig(\n",
    "        num_tags_per_key=(3, 7),\n",
    "        unique_tags_range=(1000, 5000),\n",
    "        distribution=TagDistribution.UNIFORM,\n",
    "        sharing_config=TagSharingConfig(\n",
    "            mode=TagSharingMode.PREFIX_SHARING,\n",
    "            shared_tags_ratio=0.5,\n",
    "            prefix_patterns=[\"tech_\", \"news_\", \"blog_\"],  # Common prefixes\n",
    "            prefix_ratio=0.6  # 60% of tags have these prefixes\n",
    "        )\n",
    "    ),\n",
    "    prefix=\"item:\",\n",
    "    key_pattern=\"item:{id}\",\n",
    "    additional_fields={\n",
    "        \"category\": lambda i: f\"cat_{i % 50}\"  # 50 categories\n",
    "    }\n",
    ")\n",
    "\n",
    "# Run async workload\n",
    "import asyncio\n",
    "\n",
    "async def run_async_example():\n",
    "    print(\"Running async prefix sharing workload...\")\n",
    "    result = await runner.run_async_workload(prefix_sharing_config, num_keys=50000, batch_size=5000)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"Total memory used: {result['memory_overhead']['total_memory_human']}\")\n",
    "    print(f\"Memory per key: {result['memory_overhead']['bytes_per_key']:.2f} bytes\")\n",
    "    print(f\"Search memory: {result['memory_overhead']['search_memory_human']}\")\n",
    "    print(f\"Index info: {result['index_info'].get('num_indexed_keys', 'N/A')} keys indexed\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the async workload\n",
    "result = asyncio.run(run_async_example())\n",
    "\n",
    "runner.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rym42bzhtwq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Benchmark Multiple Configurations\n",
    "# Compare memory usage across different configurations\n",
    "\n",
    "configurations = [\n",
    "    {\n",
    "        \"name\": \"Single Unique Tag (100 keys per tag)\",\n",
    "        \"config\": HashGeneratorConfig(\n",
    "            index_schema=IndexSchema(\n",
    "                name=\"idx_single_tag\",\n",
    "                fields=[FieldSchema(name=\"tag\", type=FieldType.TAG)],\n",
    "                on_hash=True,\n",
    "                prefixes=[\"key:\"]\n",
    "            ),\n",
    "            tags_config=TagsConfig(\n",
    "                num_tags_per_key=(1, 1),  # Exactly 1 tag per key\n",
    "                unique_tags_range=(100, 100),  # 100 unique tags\n",
    "                distribution=TagDistribution.UNIFORM\n",
    "            ),\n",
    "            prefix=\"key:\",\n",
    "            key_pattern=\"key:{id}\"\n",
    "        ),\n",
    "        \"num_keys\": 10000\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Low Tag Sharing\",\n",
    "        \"config\": HashGeneratorConfig(\n",
    "            index_schema=IndexSchema(\n",
    "                name=\"idx_low_share\",\n",
    "                fields=[FieldSchema(name=\"tags\", type=FieldType.TAG, separator=\"|\")],\n",
    "                on_hash=True,\n",
    "                prefixes=[\"key:\"]\n",
    "            ),\n",
    "            tags_config=TagsConfig(\n",
    "                num_tags_per_key=(3, 5),\n",
    "                unique_tags_range=(10000, 50000),\n",
    "                distribution=TagDistribution.UNIFORM,\n",
    "                sharing_config=TagSharingConfig(\n",
    "                    mode=TagSharingMode.LOW_SHARING,\n",
    "                    shared_tags_ratio=0.1\n",
    "                )\n",
    "            ),\n",
    "            prefix=\"key:\",\n",
    "            key_pattern=\"key:{id}\"\n",
    "        ),\n",
    "        \"num_keys\": 10000\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"High Tag Sharing\",\n",
    "        \"config\": HashGeneratorConfig(\n",
    "            index_schema=IndexSchema(\n",
    "                name=\"idx_high_share\",\n",
    "                fields=[FieldSchema(name=\"tags\", type=FieldType.TAG, separator=\"|\")],\n",
    "                on_hash=True,\n",
    "                prefixes=[\"key:\"]\n",
    "            ),\n",
    "            tags_config=TagsConfig(\n",
    "                num_tags_per_key=(5, 10),\n",
    "                unique_tags_range=(100, 500),\n",
    "                distribution=TagDistribution.ZIPFIAN,\n",
    "                sharing_config=TagSharingConfig(\n",
    "                    mode=TagSharingMode.HIGH_SHARING,\n",
    "                    shared_tags_ratio=0.8\n",
    "                )\n",
    "            ),\n",
    "            prefix=\"key:\",\n",
    "            key_pattern=\"key:{id}\"\n",
    "        ),\n",
    "        \"num_keys\": 10000\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run benchmarks\n",
    "results = []\n",
    "for config_info in configurations:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Running: {config_info['name']}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    result = runner.run_sync_workload(config_info['config'], config_info['num_keys'])\n",
    "    result['name'] = config_info['name']\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"Memory per key: {result['memory_overhead']['bytes_per_key']:.2f} bytes\")\n",
    "    print(f\"Total memory: {result['memory_overhead']['total_memory_human']}\")\n",
    "    \n",
    "    runner.cleanup()\n",
    "\n",
    "# Summary comparison\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"{'Configuration':<30} {'Bytes/Key':<15} {'Total Memory':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for result in results:\n",
    "    print(f\"{result['name']:<30} {result['memory_overhead']['bytes_per_key']:<15.2f} {result['memory_overhead']['total_memory_human']:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uyneuu5nq3o",
   "metadata": {},
   "source": [
    "## Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eesh3xol7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Visualization Support (requires matplotlib)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create bar chart of memory usage\n",
    "    if results:\n",
    "        names = [r['name'] for r in results]\n",
    "        bytes_per_key = [r['memory_overhead']['bytes_per_key'] for r in results]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(names, bytes_per_key)\n",
    "        plt.xlabel('Configuration')\n",
    "        plt.ylabel('Bytes per Key')\n",
    "        plt.title('Memory Usage Comparison Across Tag Configurations')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"matplotlib not installed. Install with: pip install matplotlib\")\n",
    "    print(\"Skipping visualization...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pjzztd3zbp",
   "metadata": {},
   "source": [
    "## Advanced Usage: Custom Workloads and Tag Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peq1mhztpyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Custom Workload with Vector Fields\n",
    "# Example showing how to create a workload with vector similarity search\n",
    "\n",
    "# Configuration with vectors and tags\n",
    "vector_config = HashGeneratorConfig(\n",
    "    index_schema=IndexSchema(\n",
    "        name=\"idx_vector\",\n",
    "        fields=[\n",
    "            FieldSchema(name=\"tags\", type=FieldType.TAG, separator=\"|\"),\n",
    "            VectorFieldSchema(\n",
    "                name=\"embedding\",\n",
    "                algorithm=VectorAlgorithm.HNSW,\n",
    "                dimension=128,\n",
    "                metric=VectorMetric.L2,\n",
    "                data_type=\"FLOAT32\"\n",
    "            )\n",
    "        ],\n",
    "        on_hash=True,\n",
    "        prefixes=[\"doc:\"]\n",
    "    ),\n",
    "    tags_config=TagsConfig(\n",
    "        num_tags_per_key=(2, 4),\n",
    "        unique_tags_range=(50, 200),\n",
    "        distribution=TagDistribution.UNIFORM\n",
    "    ),\n",
    "    prefix=\"doc:\",\n",
    "    key_pattern=\"doc:{id}\",\n",
    "    # Vector generation happens automatically based on schema\n",
    ")\n",
    "\n",
    "print(\"Running vector + tags workload...\")\n",
    "result = runner.run_sync_workload(vector_config, num_keys=1000)\n",
    "\n",
    "print(f\"\\nResults with vectors:\")\n",
    "print(f\"Total memory used: {result['memory_overhead']['total_memory_human']}\")\n",
    "print(f\"Memory per key: {result['memory_overhead']['bytes_per_key']:.2f} bytes\")\n",
    "print(f\"Search memory: {result['memory_overhead']['search_memory_human']}\")\n",
    "\n",
    "runner.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l4nd7hmue4s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Memory Monitoring During Insertion\n",
    "# Monitor memory growth during data insertion\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def monitor_memory_during_insertion(config, num_keys, sample_interval=1000):\n",
    "    \"\"\"Monitor memory usage during insertion\"\"\"\n",
    "    memory_samples = []\n",
    "    \n",
    "    # Create index\n",
    "    index_name = config.index_schema.name\n",
    "    create_cmd = runner._build_create_index_command(config.index_schema)\n",
    "    runner.client.execute_command(*create_cmd)\n",
    "    \n",
    "    # Get initial memory\n",
    "    initial = get_memory_info(runner.client)\n",
    "    memory_samples.append({\n",
    "        'keys': 0,\n",
    "        'memory_mb': initial.used_memory / (1024 * 1024),\n",
    "        'search_memory_mb': initial.used_memory_search / (1024 * 1024)\n",
    "    })\n",
    "    \n",
    "    # Generate and insert data with monitoring\n",
    "    generator = HashKeyGenerator(config)\n",
    "    \n",
    "    for i in range(0, num_keys, sample_interval):\n",
    "        # Insert batch\n",
    "        batch_end = min(i + sample_interval, num_keys)\n",
    "        batch_data = list(generator.generate_range(i, batch_end))\n",
    "        \n",
    "        pipe = runner.client.pipeline(transaction=False)\n",
    "        for key, hash_data in batch_data:\n",
    "            pipe.hset(key, mapping=hash_data)\n",
    "        pipe.execute()\n",
    "        \n",
    "        # Sample memory\n",
    "        current = get_memory_info(runner.client)\n",
    "        memory_samples.append({\n",
    "            'keys': batch_end,\n",
    "            'memory_mb': current.used_memory / (1024 * 1024),\n",
    "            'search_memory_mb': current.used_memory_search / (1024 * 1024)\n",
    "        })\n",
    "        \n",
    "        print(f\"Inserted {batch_end} keys - Memory: {format_bytes(current.used_memory)}\")\n",
    "    \n",
    "    runner.cleanup()\n",
    "    return pd.DataFrame(memory_samples)\n",
    "\n",
    "# Monitor memory growth\n",
    "print(\"Monitoring memory growth during insertion...\")\n",
    "df = monitor_memory_during_insertion(low_sharing_config, num_keys=20000, sample_interval=2000)\n",
    "\n",
    "# Plot if matplotlib is available\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['keys'], df['memory_mb'], label='Total Memory', marker='o')\n",
    "    plt.plot(df['keys'], df['search_memory_mb'], label='Search Memory', marker='s')\n",
    "    plt.xlabel('Number of Keys')\n",
    "    plt.ylabel('Memory Usage (MB)')\n",
    "    plt.title('Memory Growth During Insertion')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Install matplotlib to see the plot\")\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78penlc6u4v",
   "metadata": {},
   "source": [
    "## Cleanup and Notes\n",
    "\n",
    "Remember to stop the server when you're done with your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x8hpzmw0voi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Cleanup\n",
    "# Stop the server when done\n",
    "server_manager.stop()\n",
    "\n",
    "# Notes:\n",
    "# 1. Make sure to build valkey-search with `./build.sh` before running\n",
    "# 2. Set VALKEY_SERVER_PATH if valkey-server is not in your PATH\n",
    "# 3. The notebook uses port 6380 by default to avoid conflicts\n",
    "# 4. For production benchmarks, use larger datasets and multiple runs\n",
    "# 5. Memory measurements include both Valkey overhead and search index overhead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
